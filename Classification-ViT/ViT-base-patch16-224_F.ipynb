{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ViT-based model using Transfer-Learning\n",
    "---\n",
    "#### Model: google/vit-base-patch16-224, descongelando las últimas 7 capas del encoder\n",
    "#### Epochs: 30\n",
    "#### Dataset: mages_3categories_balanced\n",
    "#### Cambios:\n",
    "- DropOut:\n",
    "    - Clasificador 0.4\n",
    "    - HiddenLayer 0.3\n",
    "    - AttentionLayer 0.3\n",
    "- Learning Rate 1e-4    "
   ],
   "id": "7022885cdaf44b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:43.586060Z",
     "start_time": "2024-10-25T15:58:43.581738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parámetros\n",
    "_model = 'google/vit-base-patch16-224'\n",
    "# path al checkpoint a cargar, None si no existe\n",
    "_checkpoint = None\n",
    "_output = 'SavedModels/ViT-base-patch16-224_F'  # path para guardar el modelo -> Revisar\n",
    "\n",
    "_dataset = '/Users/julio/Documentos-Local/data/VinDr-Mammo/subsets/images_3categories_balanced'\n",
    "\n",
    "# Dataset aumentado\n",
    "_dataset_augmented_path = '/Users/julio/Documentos-Local/data/VinDr-Mammo/subsets/images_3categories_balanced_augmented'\n",
    "\n",
    "# path para guardar el dataset con split\n",
    "_dataset_split_path = '/Users/julio/Documentos-Local/data/VinDr-Mammo/subsets/images_3categories_balanced_augmented_split'\n",
    "\n",
    "# Si el dataset ya está separado en train, validation y test ->  _dataset_split=_dataset_split_path. \n",
    "# Si no está separado -> _dataset_split=None.\n",
    "_dataset_split = None  \n",
    "\n",
    "_batch_size = 32\n",
    "_learning_rate = 1e-4\n",
    "_epochs = 30  # Epochs a entrenar el modelo\n",
    "\n",
    "# DropOut\n",
    "_dp_clasificador = 0.4\n",
    "_dp_hidden_layer = 0.3\n",
    "_dp_attention_layer= 0.3\n",
    "\n",
    "num_layers_to_unfreeze = 7  # Definir el número de capas a descongelar, None eoc"
   ],
   "id": "70a22524132806c4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:46.092349Z",
     "start_time": "2024-10-25T15:58:43.588434Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, DatasetDict, load_from_disk,concatenate_datasets\n",
    "\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "import evaluate\n",
    "from torchvision import transforms\n",
    "from Utils import *"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Carga de datos",
   "id": "86c5c376167db015"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:49.292325Z",
     "start_time": "2024-10-25T15:58:46.181421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if _dataset_split is None:\n",
    "    dataset = load_dataset(_dataset)\n",
    "else:\n",
    "    # Cargar el dataset previamente guardado\n",
    "    dataset = load_from_disk(_dataset_split_path)\n",
    "\n",
    "dataset"
   ],
   "id": "3cc4f4afb99d5cf2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/7504 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2513d254260432eac61cc2acd4e658b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 7504\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extra Data Augmentation",
   "id": "18c8cb2da4f31c64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:49.402300Z",
     "start_time": "2024-10-25T15:58:49.301236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definir transformaciones de Data Augmentation para el entrenamiento\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),                # Rotación aleatoria de ±10 grados\n",
    "    transforms.RandomHorizontalFlip(),            # Flip horizontal aleatorio\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Cambios aleatorios de brillo, contraste, saturación y tono\n",
    "])\n",
    "\n",
    "def augment_images(batch):\n",
    "    \"\"\"Aplica transformaciones de Data Augmentation a las imágenes del batch.\"\"\"\n",
    "    augmented_images = []\n",
    "    for img in batch['image']:\n",
    "        # Convertir PIL Image a RGB\n",
    "        img = img.convert('RGB')\n",
    "        # Aplicar augmentations\n",
    "        img = augmentation_transforms(img)\n",
    "        augmented_images.append(img)\n",
    "    batch['image'] = augmented_images\n",
    "    return batch\n",
    "\n",
    "# Aplicar Data Augmentation solo al conjunto de entrenamiento\n",
    "augmented_train = dataset['train'].map(augment_images, batched=True)\n",
    "\n",
    "# Concatenar el dataset original de entrenamiento con el aumentado\n",
    "# Esto duplica el tamaño del conjunto de entrenamiento con datos aumentados\n",
    "augmented_train = concatenate_datasets([dataset['train'], augmented_train])\n",
    "dataset = augmented_train\n"
   ],
   "id": "96f8b7de03838453",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Revisión de categorías",
   "id": "74ee833516aa9480"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:49.415782Z",
     "start_time": "2024-10-25T15:58:49.413646Z"
    }
   },
   "cell_type": "code",
   "source": "dataset",
   "id": "d18ba6225869f689",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 15008\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:49.484317Z",
     "start_time": "2024-10-25T15:58:49.482147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = dataset.features['label'].names\n",
    "print(len(labels),labels)\n",
    "\n",
    "label2id = {c:idx for idx,c in enumerate(labels)}\n",
    "id2label = {idx:c for idx,c in enumerate(labels)}"
   ],
   "id": "672104ff2885bdeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['calcificaciones', 'masas', 'no_encontrado']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Muestra de ejemplos",
   "id": "8a5e0a4c31f70014"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:49.550630Z",
     "start_time": "2024-10-25T15:58:49.548087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_samples(ds,rows,cols):\n",
    "    samples = ds.shuffle().select(np.arange(rows*cols)) # selecting random images\n",
    "    fig = plt.figure(figsize=(cols*4,rows*4))\n",
    "    # plotting\n",
    "    for i in range(rows*cols):\n",
    "        img = samples[i]['image']\n",
    "        label = samples[i]['label']\n",
    "        fig.add_subplot(rows,cols,i+1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(label)\n",
    "        plt.axis('off')\n",
    "            \n",
    "# show_samples(dataset['train'],rows=3,cols=5)"
   ],
   "id": "2e09d39396d652ae",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset",
   "id": "65cd6f013528a8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:58:51.256089Z",
     "start_time": "2024-10-25T15:58:49.569890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if _dataset_split is None:\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "    eval_dataset = split_dataset['test'].train_test_split(test_size=0.5)\n",
    "    \n",
    "    \n",
    "    # Recombinar los splits \n",
    "    \n",
    "    final_dataset = DatasetDict({\n",
    "        'train': split_dataset['train'],\n",
    "        'validation': eval_dataset['train'],\n",
    "        'test': eval_dataset['test']\n",
    "    })\n",
    "    # Guardar el dataset dividido\n",
    "    final_dataset.save_to_disk(_dataset_split_path)\n",
    "\n",
    "else:\n",
    "    final_dataset = dataset\n",
    "final_dataset"
   ],
   "id": "826ff9fdce47b2e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12006 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0378b85ea272416ea11dbae946eba4ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1501 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39177ddbae7e4b31a9e34a1e8970a512"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1501 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a728b01ba3704c87b0251ac7eaabd3db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 12006\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1501\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1501\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:02.351498Z",
     "start_time": "2024-10-25T15:58:51.276846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Número de imágenes por clases en cada split')\n",
    "clases_split = pd.DataFrame(columns=['split', *labels])\n",
    "for key in final_dataset:\n",
    "    split = pd.DataFrame(final_dataset[key])\n",
    "    num = split['label'].value_counts().sort_index()\n",
    "    clases_split.loc[len(clases_split)] = [key, *num]\n",
    "    #print(num.sort_index())\n",
    "clases_split"
   ],
   "id": "7c0520fb8246ae8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes por clases en cada split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "        split  calcificaciones  masas  no_encontrado\n",
       "0       train             4023   3973           4010\n",
       "1  validation              462    518            521\n",
       "2        test              519    513            469"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>calcificaciones</th>\n",
       "      <th>masas</th>\n",
       "      <th>no_encontrado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>4023</td>\n",
       "      <td>3973</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation</td>\n",
       "      <td>462</td>\n",
       "      <td>518</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>519</td>\n",
       "      <td>513</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocesamiento de las imágenes",
   "id": "54d16d02617d0642"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:03.112004Z",
     "start_time": "2024-10-25T15:59:02.377668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = AutoImageProcessor.from_pretrained(_model, use_fast=True)\n",
    "processor"
   ],
   "id": "1c2cd9e3e4ab5a4a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessorFast {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessorFast\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:03.136636Z",
     "start_time": "2024-10-25T15:59:03.134219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transforms(batch):\n",
    "    batch['image'] = [x.convert('RGB') for x in batch['image']]\n",
    "    inputs = processor(batch['image'],return_tensors='pt')\n",
    "    inputs['labels'] = batch['label']  # Las clases ya están en formato numérico\n",
    "    return inputs"
   ],
   "id": "fd8e91d20395f822",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:03.204719Z",
     "start_time": "2024-10-25T15:59:03.173248Z"
    }
   },
   "cell_type": "code",
   "source": "processed_dataset = final_dataset.with_transform(transforms)",
   "id": "7fd84c61b445fd27",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Collation",
   "id": "3c20e0c2fcab0cbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:03.209267Z",
     "start_time": "2024-10-25T15:59:03.207452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ],
   "id": "b594c7a07efc3686",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Métricas de evaluación",
   "id": "248fd8bb2ba4efb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:07.773974Z",
     "start_time": "2024-10-25T15:59:03.215597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    print(\"Conteo de etiquetas reales:\", Counter(labels))\n",
    "    print(\"Conteo de predicciones:\", Counter(predictions))\n",
    "    \n",
    "    precision_per_class = precision.compute(\n",
    "    predictions=predictions, references=labels, average=None)\n",
    "    print(\"Precision por clase:\", precision_per_class, '\\n')\n",
    "\n",
    "    # Accuracy no requiere el parámetro average\n",
    "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    \n",
    "    # Las demás métricas sí requieren el parámetro average para multiclase\n",
    "    precision_score = precision.compute(predictions=predictions, references=labels, average='macro')['precision']\n",
    "    recall_score = recall.compute(predictions=predictions, references=labels, average='macro')['recall']\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average='macro')['f1']\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score,\n",
    "        'precision': precision_score,\n",
    "        'recall': recall_score,\n",
    "        'f1': f1_score\n",
    "    }"
   ],
   "id": "eeb8010325f0f0cc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Carga del modelo",
   "id": "952d173ae5c464b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.273861Z",
     "start_time": "2024-10-25T15:59:07.789566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clase personalizada que añade dropout antes de la capa final de clasificación\n",
    "class CustomViTForImageClassification(ViTForImageClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # Dropout adicional antes de la capa final\n",
    "        self.additional_dropout = nn.Dropout(_dp_clasificador)  # Dropout antes del clasificador\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.vit(pixel_values)  # Obtenemos la salida del modelo ViT\n",
    "        \n",
    "        # Usamos el primer token [CLS] de la salida\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] está en la posición 0\n",
    "        \n",
    "        # Aplicamos dropout adicional antes de la clasificación\n",
    "        pooled_output = self.additional_dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Asegúrate de que las etiquetas sean tipo long (para clasificación)\n",
    "            class_weights = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32)  # Aumentar peso de \"sospechoso\"\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(self.device))  # Enviar los pesos al dispositivo\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Configuramos el modelo base con Dropout en las capas internas del ViT\n",
    "model = CustomViTForImageClassification.from_pretrained(\n",
    "    _model,\n",
    "    num_labels = len(labels),\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    hidden_dropout_prob=_dp_hidden_layer,  # Dropout en las capas internas del modelo\n",
    "    attention_probs_dropout_prob=_dp_attention_layer,  # Dropout en las capas de atención\n",
    "    ignore_mismatched_sizes = True\n",
    ")"
   ],
   "id": "90238aad36c0de2b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Arquitectura del modelo",
   "id": "922184390f5b66e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.291579Z",
     "start_time": "2024-10-25T15:59:09.288715Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "46712562a7213709",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.3, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (additional_dropout): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Congelar todas las capas, menos el clasificador",
   "id": "de3fe87175e49406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.312339Z",
     "start_time": "2024-10-25T15:59:09.309793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name,p in model.named_parameters():\n",
    "    if not name.startswith('classifier'):\n",
    "        p.requires_grad = False"
   ],
   "id": "7b12d0b8faee7367",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.339660Z",
     "start_time": "2024-10-25T15:59:09.336797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "print(f\"{num_params = :,} | {trainable_params = :,}\")"
   ],
   "id": "64c82a89eda67be7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params = 85,800,963 | trainable_params = 2,307\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Descongelar capas del encoder para fine-tuning",
   "id": "5ed4701c8e3fa107"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.362636Z",
     "start_time": "2024-10-25T15:59:09.360715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Obtener el número total de capas en el encoder\n",
    "num_total_layers = len(list(model.vit.encoder.layer))  # Debería ser 24 para ViT-Large, 12 para ViT-base\n",
    "print(num_total_layers)"
   ],
   "id": "94cb222cd820fbde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.392033Z",
     "start_time": "2024-10-25T15:59:09.389459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Si se descongelan capas\n",
    "if num_layers_to_unfreeze is not None:\n",
    "    # Calcular el índice a partir del cual descongelar\n",
    "    unfreeze_from = num_total_layers - num_layers_to_unfreeze\n",
    "    \n",
    "    # Iterar sobre todas las capas del encoder\n",
    "    for idx, layer in enumerate(model.vit.encoder.layer):\n",
    "        if idx >= unfreeze_from:\n",
    "            # Descongelar esta capa\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Congelar esta capa\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n"
   ],
   "id": "7b3a8cb360656483",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.412251Z",
     "start_time": "2024-10-25T15:59:09.409212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Mostrar el número total de parámetros y los entrenables después de descongelar\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Después de descongelar las últimas {num_layers_to_unfreeze} capas:\")\n",
    "print(f\"Total de parámetros: {num_params:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params:,}\")"
   ],
   "id": "e145df6d86c1cc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de descongelar las últimas 7 capas:\n",
      "Total de parámetros: 85,800,963\n",
      "Parámetros entrenables: 49,617,411\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.437040Z",
     "start_time": "2024-10-25T15:59:09.434916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Revisión de trainable por capa\n",
    "for name, param in model.named_parameters():\n",
    "    status = \"Trainable\" if param.requires_grad else \"Frozen\"\n",
    "    #print(f\"{name}: {status}\")"
   ],
   "id": "f7dfb567949b7bbf",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "19a3a741baf42170"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.457891Z",
     "start_time": "2024-10-25T15:59:09.445031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=_output,\n",
    "    per_device_train_batch_size=_batch_size,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=_epochs,  # Epochs a entrenar -> Revisar\n",
    "    learning_rate=_learning_rate,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"precision\",  # Métrica para seleccionar el mejor modelo\n",
    "    greater_is_better=True  # True si la métrica debe aumentar (ej., accuracy)\n",
    ")"
   ],
   "id": "541994dbbce332e6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.466575Z",
     "start_time": "2024-10-25T15:59:09.465113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Crear el callback de early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Número de epochs sin mejora permitida\n",
    "    early_stopping_threshold=0.0001  # Mejora mínima necesaria\n",
    ")\n"
   ],
   "id": "56f8a560451e5ea3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:09.986264Z",
     "start_time": "2024-10-25T15:59:09.473621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    "    tokenizer=processor,\n",
    "    #callbacks=[early_stopping_callback]  # Añadir el callback\n",
    ")"
   ],
   "id": "30f59f8a406a04ef",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T15:59:10.057143Z",
     "start_time": "2024-10-25T15:59:10.055032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check that MPS is available\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"MPS enabled\")"
   ],
   "id": "56a200df21ff907e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:28:26.164649Z",
     "start_time": "2024-10-25T15:59:25.409308Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "f3df38c3b841d8a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8278' max='11280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8278/11280 2:28:58 < 54:02, 0.93 it/s, Epoch 22.01/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>0.878999</td>\n",
       "      <td>0.596269</td>\n",
       "      <td>0.597950</td>\n",
       "      <td>0.592128</td>\n",
       "      <td>0.561466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.767500</td>\n",
       "      <td>0.802598</td>\n",
       "      <td>0.634244</td>\n",
       "      <td>0.640682</td>\n",
       "      <td>0.634030</td>\n",
       "      <td>0.597588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.743400</td>\n",
       "      <td>0.781688</td>\n",
       "      <td>0.646236</td>\n",
       "      <td>0.649354</td>\n",
       "      <td>0.647660</td>\n",
       "      <td>0.608331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.648235</td>\n",
       "      <td>0.652748</td>\n",
       "      <td>0.649783</td>\n",
       "      <td>0.614311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.704600</td>\n",
       "      <td>0.843321</td>\n",
       "      <td>0.640240</td>\n",
       "      <td>0.649766</td>\n",
       "      <td>0.644442</td>\n",
       "      <td>0.596402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.763461</td>\n",
       "      <td>0.661559</td>\n",
       "      <td>0.658785</td>\n",
       "      <td>0.663106</td>\n",
       "      <td>0.637581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.771933</td>\n",
       "      <td>0.676882</td>\n",
       "      <td>0.677273</td>\n",
       "      <td>0.678096</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>0.801289</td>\n",
       "      <td>0.665556</td>\n",
       "      <td>0.670420</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>0.643310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641200</td>\n",
       "      <td>0.786020</td>\n",
       "      <td>0.666223</td>\n",
       "      <td>0.670273</td>\n",
       "      <td>0.667518</td>\n",
       "      <td>0.643066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.841269</td>\n",
       "      <td>0.667555</td>\n",
       "      <td>0.682578</td>\n",
       "      <td>0.668244</td>\n",
       "      <td>0.638389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.802905</td>\n",
       "      <td>0.667555</td>\n",
       "      <td>0.677068</td>\n",
       "      <td>0.669786</td>\n",
       "      <td>0.638008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>0.905274</td>\n",
       "      <td>0.666223</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>0.669646</td>\n",
       "      <td>0.637390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.839318</td>\n",
       "      <td>0.660893</td>\n",
       "      <td>0.672009</td>\n",
       "      <td>0.664375</td>\n",
       "      <td>0.633976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.584700</td>\n",
       "      <td>0.828835</td>\n",
       "      <td>0.674883</td>\n",
       "      <td>0.675176</td>\n",
       "      <td>0.673343</td>\n",
       "      <td>0.656788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.576700</td>\n",
       "      <td>0.911042</td>\n",
       "      <td>0.673551</td>\n",
       "      <td>0.677150</td>\n",
       "      <td>0.674184</td>\n",
       "      <td>0.652026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.931621</td>\n",
       "      <td>0.665556</td>\n",
       "      <td>0.676156</td>\n",
       "      <td>0.666930</td>\n",
       "      <td>0.639951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>1.077622</td>\n",
       "      <td>0.648901</td>\n",
       "      <td>0.664309</td>\n",
       "      <td>0.645535</td>\n",
       "      <td>0.625696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.912742</td>\n",
       "      <td>0.672885</td>\n",
       "      <td>0.684251</td>\n",
       "      <td>0.673738</td>\n",
       "      <td>0.647748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.935073</td>\n",
       "      <td>0.668221</td>\n",
       "      <td>0.672551</td>\n",
       "      <td>0.668902</td>\n",
       "      <td>0.647082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>1.034572</td>\n",
       "      <td>0.671552</td>\n",
       "      <td>0.672558</td>\n",
       "      <td>0.672450</td>\n",
       "      <td>0.651811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>1.027500</td>\n",
       "      <td>0.661559</td>\n",
       "      <td>0.671904</td>\n",
       "      <td>0.665639</td>\n",
       "      <td>0.639789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>1.080101</td>\n",
       "      <td>0.656895</td>\n",
       "      <td>0.671710</td>\n",
       "      <td>0.665478</td>\n",
       "      <td>0.637327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 876, np.int64(0): 349, np.int64(1): 276})\n",
      "Precision por clase: {'precision': array([0.66475645, 0.54347826, 0.58561644])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 835, np.int64(0): 449, np.int64(1): 217})\n",
      "Precision por clase: {'precision': array([0.66592428, 0.640553  , 0.61556886])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 794, np.int64(0): 500, np.int64(1): 207})\n",
      "Precision por clase: {'precision': array([0.65      , 0.65700483, 0.64105793])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 766, np.int64(0): 517, np.int64(1): 218})\n",
      "Precision por clase: {'precision': array([0.63249516, 0.67431193, 0.65143603])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 743, np.int64(0): 589, np.int64(1): 169})\n",
      "Precision por clase: {'precision': array([0.60611205, 0.68639053, 0.65679677])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 704, np.int64(0): 526, np.int64(1): 271})\n",
      "Precision por clase: {'precision': array([0.63117871, 0.66051661, 0.68465909])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 698, np.int64(0): 496, np.int64(1): 307})\n",
      "Precision por clase: {'precision': array([0.6733871 , 0.68078176, 0.67765043])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 783, np.int64(0): 422, np.int64(1): 296})\n",
      "Precision por clase: {'precision': array([0.71090047, 0.65540541, 0.6449553 ])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 737, np.int64(0): 497, np.int64(1): 267})\n",
      "Precision por clase: {'precision': array([0.66599598, 0.68539326, 0.65943012])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 803, np.int64(0): 465, np.int64(1): 233})\n",
      "Precision por clase: {'precision': array([0.69892473, 0.71244635, 0.63636364])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 746, np.int64(0): 527, np.int64(1): 228})\n",
      "Precision por clase: {'precision': array([0.65275142, 0.71491228, 0.66353887])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 724, np.int64(0): 564, np.int64(1): 213})\n",
      "Precision por clase: {'precision': array([0.63475177, 0.76056338, 0.66298343])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 702, np.int64(0): 570, np.int64(1): 229})\n",
      "Precision por clase: {'precision': array([0.6245614 , 0.72052402, 0.67094017])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 747, np.int64(0): 421, np.int64(1): 333})\n",
      "Precision por clase: {'precision': array([0.71021378, 0.64864865, 0.66666667])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 743, np.int64(0): 476, np.int64(1): 282})\n",
      "Precision por clase: {'precision': array([0.68487395, 0.68439716, 0.66218035])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 758, np.int64(0): 497, np.int64(1): 246})\n",
      "Precision por clase: {'precision': array([0.66800805, 0.71138211, 0.64907652])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 838, np.int64(0): 356, np.int64(1): 307})\n",
      "Precision por clase: {'precision': array([0.74438202, 0.63517915, 0.61336516])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 778, np.int64(0): 470, np.int64(1): 253})\n",
      "Precision por clase: {'precision': array([0.7       , 0.70750988, 0.64524422])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 736, np.int64(0): 485, np.int64(1): 280})\n",
      "Precision por clase: {'precision': array([0.66804124, 0.68928571, 0.66032609])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 711, np.int64(0): 499, np.int64(1): 291})\n",
      "Precision por clase: {'precision': array([0.65731463, 0.6838488 , 0.67651195])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 650, np.int64(0): 602, np.int64(1): 249})\n",
      "Precision por clase: {'precision': array([0.60299003, 0.7188755 , 0.69384615])} \n",
      "\n",
      "Conteo de etiquetas reales: Counter({np.int64(2): 521, np.int64(1): 518, np.int64(0): 462})\n",
      "Conteo de predicciones: Counter({np.int64(2): 753, np.int64(0): 476, np.int64(1): 272})\n",
      "Precision por clase: {'precision': array([0.68487395, 0.68014706, 0.64807437])} \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PMM/Codigos/Test1/Classification-ViT/.venv/lib/python3.11/site-packages/transformers/trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2054\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/PMM/Codigos/Test1/Classification-ViT/.venv/lib/python3.11/site-packages/transformers/trainer.py:2393\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2387\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m   2388\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   2390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2391\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2392\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m-> 2393\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misinf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss_step\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   2394\u001B[0m ):\n\u001B[1;32m   2395\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2396\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n\u001B[1;32m   2397\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.save_model()",
   "id": "7fb997a65c2644c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluación del modelo",
   "id": "7a9240869110b2e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T18:29:19.741570Z",
     "start_time": "2024-10-25T18:28:52.403728Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate(processed_dataset['test'])",
   "id": "bca36b8e1d96cbe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteo de etiquetas reales: Counter({np.int64(0): 519, np.int64(1): 513, np.int64(2): 469})\n",
      "Conteo de predicciones: Counter({np.int64(2): 737, np.int64(0): 500, np.int64(1): 264})\n",
      "Precision por clase: {'precision': array([0.7       , 0.70454545, 0.61058345])} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0801012516021729,\n",
       " 'eval_accuracy': 0.6568954030646236,\n",
       " 'eval_precision': 0.6717096336499321,\n",
       " 'eval_recall': 0.6654783893657975,\n",
       " 'eval_f1': 0.6373270412348818}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inferencia en conjunto de test ",
   "id": "b0e855e955ca27b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "samples = final_dataset['test']\n",
    "processed_samples = samples.with_transform(transforms)\n",
    "predictions = trainer.predict(processed_samples).predictions.argmax(axis=1) # labels predichas"
   ],
   "id": "f37bb4fbf9efd886",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_predictions(rows=5,cols=5, samples_=samples, predictions_=predictions, id2label_=id2label)",
   "id": "12a188f84c9f3779",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Matriz de confusión",
   "id": "97916219ffa54a0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_matrix(samples_=samples, predictions_=predictions, class_names=labels)",
   "id": "f6f7fbb54f99d3f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Iterar por más epochs ❌",
   "id": "4cb217670e463742"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.args.num_train_epochs = 30  # Para entrenar hasta la época 20\n",
    "trainer.train(resume_from_checkpoint=_checkpoint)"
   ],
   "id": "6f532223cd642edd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.save_model()",
   "id": "81df7794f48301f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.evaluate(processed_dataset['test'])",
   "id": "e34356d41f6d33da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "confusion_matrix(samples_=samples, predictions_=predictions)",
   "id": "1827c30c0aae817e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "139cfc3ed6ac8ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
